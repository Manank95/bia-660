{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manan\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10, perplexity: 1523.1468\n",
      "iteration: 2 of max_iter: 10, perplexity: 1507.5188\n",
      "iteration: 3 of max_iter: 10, perplexity: 1503.0523\n",
      "iteration: 4 of max_iter: 10, perplexity: 1501.0292\n",
      "iteration: 5 of max_iter: 10, perplexity: 1499.8969\n",
      "iteration: 6 of max_iter: 10, perplexity: 1499.1721\n",
      "iteration: 7 of max_iter: 10, perplexity: 1498.6696\n",
      "iteration: 8 of max_iter: 10, perplexity: 1498.2985\n",
      "iteration: 9 of max_iter: 10, perplexity: 1498.0128\n",
      "iteration: 10 of max_iter: 10, perplexity: 1497.7862\n",
      "Topic 0:\n",
      "[('comment', 3603.4727314404663), ('oil', 2273.7929516913591), ('com', 2064.3345024952578), ('sign', 1927.3073851492945), ('news', 1912.9397097272627), ('users', 1616.0320178020422), ('rate', 1608.5184629872058), ('new', 1440.83527191556), ('bp', 1252.0973756188062), ('baby', 1160.5587675910365), ('report', 1080.2536868212969), ('travel', 1079.1339156942231), ('www', 1077.4169579710233), ('information', 1027.7456672106291), ('video', 1024.7169963220092), ('gulf', 953.13009248736716), ('cruise', 927.55065444347611), ('spill', 919.24677711074992), ('company', 860.95525509106835), ('ap', 851.70963145793951)]\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "[('said', 9601.9767341223123), ('people', 3630.0835631452187), ('city', 1520.8500269232804), ('japan', 1382.7169559081972), ('water', 1376.9104476282655), ('police', 1368.5901514918594), ('officials', 1322.5616272766456), ('killed', 1129.5535443455938), ('year', 1124.9597372545595), ('says', 1090.4672917750245), ('crash', 1046.7070959108805), ('told', 1041.8965149310311), ('state', 1037.6633920797024), ('safety', 984.1608989086634), ('nuclear', 952.03169354439035), ('hit', 945.80036625850096), ('government', 942.43085282478944), ('plane', 942.25180482599023), ('area', 938.19437971798675), ('new', 926.73357415401688)]\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "[('said', 5974.1842919509018), ('percent', 4247.4399815410225), ('year', 3466.5923982266036), ('tax', 3000.4984723839684), ('government', 2371.150775137995), ('new', 2197.5009460240317), ('obama', 2150.7893186803058), ('economy', 2093.291744700211), ('billion', 2027.5388958309907), ('economic', 1799.0846084795864), ('million', 1703.6945931208122), ('president', 1457.027653456666), ('money', 1435.1495537379299), ('high', 1396.8328812273255), ('market', 1368.4650352932335), ('state', 1330.4812064162936), ('budget', 1297.0585510461258), ('rail', 1266.572412703267), ('jobs', 1265.1409262577176), ('years', 1262.68716264057)]\n",
      "\n",
      "\n",
      "[[ 0.02366008  0.97530542  0.00103449]\n",
      " [ 0.60465828  0.39326591  0.00207581]\n",
      " [ 0.02332013  0.02295517  0.9537247 ]\n",
      " [ 0.00100011  0.99308952  0.00591037]\n",
      " [ 0.34835385  0.60740987  0.04423628]]\n",
      "\n",
      "\n",
      "\n",
      "[1, 0, 2, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "  actual_class  topics\n",
      "0           T3       1\n",
      "1           T2       0\n",
      "2           T2       2\n",
      "3           T3       1\n",
      "4           T2       1\n",
      "actual_class   T1   T2   T3\n",
      "topics                     \n",
      "0             190  194  186\n",
      "1             646  532  495\n",
      "2             501  458  416\n",
      "\n",
      "\n",
      "Task 2: (c) ----------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         T1       0.33      0.14      0.20      1337\n",
      "         T2       0.32      0.45      0.37      1184\n",
      "         T3       0.30      0.38      0.34      1097\n",
      "\n",
      "avg / total       0.32      0.31      0.30      3618\n",
      "\n",
      "\n",
      "\n",
      "Task 2: (d) ----------\n",
      "\n",
      "meaningful names to each cluster based on cluster centroids/samples.\n",
      "Cluster 0: com.oil.news (T1)\n",
      "Cluster 1: com.accidents (T2)\n",
      "Cluster 2: com.economy.report (T3)\n",
      "\n",
      "\n",
      "Task 3 (1) - applying threshold\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "\n",
      "Task 3 (2) - Applying multilabel classification for all-labels\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         T1       0.91      0.86      0.88       445\n",
      "         T2       0.98      0.90      0.94       419\n",
      "         T3       0.88      0.86      0.87       392\n",
      "\n",
      "avg / total       0.92      0.88      0.90      1256\n",
      "\n",
      "0.920119877346\n",
      "0.875022342208\n",
      "0.896837838529\n",
      "\n",
      "\n",
      "Determine the best threshold from 0 to 1\n",
      "Threshold =  0\n",
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "\n",
      "\n",
      "Threshold =  0.05\n",
      "[[0 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.1\n",
      "[[0 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.15000000000000002\n",
      "[[0 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.2\n",
      "[[0 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.25\n",
      "[[0 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.3\n",
      "[[0 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.35\n",
      "[[0 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.39999999999999997\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.44999999999999996\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.49999999999999994\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.5499999999999999\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.6\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.65\n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.7000000000000001\n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.7500000000000001\n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.8000000000000002\n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.8500000000000002\n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.9000000000000002\n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "\n",
      "\n",
      "Threshold =  0.9500000000000003\n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#assignment 7\n",
    "# Task 1 - K-mean clustering.\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.cluster import KMeansClusterer, cosine_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# for task 2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def task1():\n",
    "    # initialize the TfidfVectorizer \n",
    "    # set min document frequency to 5\n",
    "    tfidf_vect = TfidfVectorizer(stop_words=\"english\", min_df=5)\n",
    "    dtm= tfidf_vect.fit_transform(text)\n",
    "\n",
    "    clusterer = KMeansClusterer(num_clusters, cosine_distance, repeats=10)\n",
    "    clusters = clusterer.cluster(dtm.toarray(), assign_clusters=True)\n",
    "    print(\"Task 1: (a)-----------\")\n",
    "    print(clusters[0:5])\n",
    "\n",
    "    # get the centroids, means to what these clusters are about and we can give meaningful names\n",
    "    centroids=np.array(clusterer.means())\n",
    "    # sort in reverese order and get feature names\n",
    "    sorted_centroids = centroids.argsort()[:, ::-1]\n",
    "    voc_lookup= tfidf_vect.get_feature_names()\n",
    "\n",
    "    print(\"\\nTo get top 20 words from each cluster.\")\n",
    "    for i in range(num_clusters):\n",
    "        # get words with top 20 tf-idf weight in the centroid\n",
    "        top_words=[voc_lookup[word_index] for word_index in sorted_centroids[i, :20]]\n",
    "        print(\"Cluster %d: %s \" % (i, \"; \".join(top_words)))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    #external evaluation/ majority vote rule\n",
    "    df=pd.DataFrame(list(zip(first_label, clusters)), columns=['actual_class','cluster'])\n",
    "    print(\"Task 1: (b)-----------\\n\")\n",
    "    print(df.head())\n",
    "    print(pd.crosstab( index=df.cluster, columns=df.actual_class))\n",
    "\n",
    "    cluster_dict={0:'T1', 1:'T2', 2:'T3'}\n",
    "    # Assign true class to cluster\n",
    "    predicted_target=[cluster_dict[i] for i in clusters]\n",
    "    #predicted_target\n",
    "    print(\"\\n\\nTask 1: (c) ----------\\n\")\n",
    "    print(metrics.classification_report(first_label, predicted_target))\n",
    "    print(\"\\n\\nTask 1: (d) ----------\\n\")\n",
    "    print(\"meaningful names to each cluster based on cluster centroids/samples.\")\n",
    "    print(\"Cluster 0: com.accidents (T1)\\nCluster 1: com.disasters (T2)\\nCluster 2: com.economy.report (T3)\")\n",
    "    \n",
    "    \n",
    "\n",
    "def task2():\n",
    "    #Task-2 LDA (single label)\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.90, min_df=50, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(text)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    X_train, X_test = train_test_split(tf, test_size=0.1, random_state=0)\n",
    "    num_topics = 3\n",
    "    num_top_words=20\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                    max_iter=10,verbose=1,\n",
    "                                    evaluate_every=1, n_jobs=1,\n",
    "                                    random_state=0).fit(X_train)\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        # print out top 10 words per topic \n",
    "        words=[(tf_feature_names[i],topic[i]) for i in topic.argsort()[::-1][0:num_top_words]]\n",
    "        print(words)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    topics_assign=lda.transform(X_train)\n",
    "    print(topics_assign[0:5])\n",
    "    print(\"\\n\\n\")\n",
    "    #topics_assign = topics_assign[~np.all(topics_assign == 0, axis=1)]\n",
    "    #print(topics_assign[0:5])\n",
    "\n",
    "    topics_assign_list = np.argmax(topics_assign, axis=1).tolist()\n",
    "    print(topics_assign_list[0:5])\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    df=pd.DataFrame(list(zip(first_label, topics_assign_list)), columns=['actual_class','topics'])\n",
    "    print(df.head())\n",
    "    print(pd.crosstab( index=df.topics, columns=df.actual_class))\n",
    "    \n",
    "    cluster_dict={0:'T1', 1:'T2', 2:'T3'}\n",
    "    # Assign true class to cluster\n",
    "    predicted_target=[cluster_dict[i] for i in topics_assign_list]\n",
    "    #print(len(predicted_target))\n",
    "    #print(len(all_labels))\n",
    "    print(\"\\n\\nTask 2: (c) ----------\\n\")\n",
    "    print(metrics.classification_report(first_label[0:3618], predicted_target))\n",
    "    print(\"\\n\\nTask 2: (d) ----------\\n\")\n",
    "    print(\"meaningful names to each cluster based on cluster centroids/samples.\")\n",
    "    print(\"Cluster 0: com.oil.news (T1)\\nCluster 1: com.accidents (T2)\\nCluster 2: com.economy.budget (T3)\")\n",
    "    \n",
    "    \n",
    "    # Task 3-----------\n",
    "    \n",
    "    \n",
    "    print(\"\\n\\nTask 3 (1) - applying threshold\")\n",
    "    prob_threshold=0.4\n",
    "    topics=np.copy(topics_assign)\n",
    "    topics=np.where(topics>=prob_threshold, 1, 0)\n",
    "    print(topics[0:5])\n",
    "    \n",
    "    #multilabel classification\n",
    "    print(\"\\n\\nTask 3 (2) - Applying multilabel classification for all-labels\")\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    Y=mlb.fit_transform(all_labels)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(text, Y, test_size=0.3, random_state=0)\n",
    "    classifier = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=\"english\", min_df=2)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    predicted = classifier.predict(X_test)\n",
    "    print(classification_report(Y_test, predicted, target_names=mlb.classes_))\n",
    "    \n",
    "    print(precision_score(Y_test, predicted, average=\"macro\"))\n",
    "    print(recall_score(Y_test, predicted, average=\"macro\"))\n",
    "    print(f1_score(Y_test, predicted, average=\"macro\"))\n",
    "\n",
    "    \n",
    "    #print(metrics.classification_report(all_labels, predicted_target))\n",
    "\n",
    "    # Task 3 (3) determine the best threshold\n",
    "    print(\"\\n\\nDetermine the best threshold from 0 to 1\")\n",
    "    tst_thr=0\n",
    "    while tst_thr<=1:\n",
    "        topics=np.copy(topics_assign)\n",
    "        topics=np.where(topics>=tst_thr, 1, 0)\n",
    "        print(\"Threshold = \",tst_thr)\n",
    "        print(topics[0:5])\n",
    "        print(\"\\n\")\n",
    "        tst_thr+=.05\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    data=json.load(open('ydata_3group.json','r'))\n",
    "    num_clusters=3\n",
    "\n",
    "    text,first_label,all_labels=zip(*data)\n",
    "    text=list(text)\n",
    "    first_label=list(first_label)\n",
    "    all_labels = list(all_labels)\n",
    "    #rint(text[0])\n",
    "    #rint(first_label[0])\n",
    "    task1()\n",
    "    print(\"\\n\\n\")\n",
    "    task2()\n",
    "    # Task 3 is included in the last function call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
