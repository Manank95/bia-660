{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "from re import compile\n",
    "#for sorting dictionary\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens=[]\n",
    "    # write your code here\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    #print(text)\n",
    "    #pattern = r'\\w'\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip(string.punctuation) for token in tokens]\n",
    "    \n",
    "    tokens = [token for token in tokens if re.match(\"^[A-Za-z_-]*$\", token)]\n",
    "    #print(tokens)\n",
    "    # to remove punctuations from begging and starting of the tokens \n",
    "    \n",
    "    # now removing extra empty characters from tokens\n",
    "    tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "    tokens = [token for token in tokens if len(token)>1]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This analysis is for  ('senior', 'software', 'engineer')\n",
      "\n",
      "##############################################\n",
      "\n",
      "'input resume' can have more chances to become   ('senior', 'software', 'engineer')  if it has following job experience terms\n",
      "\n",
      "('senior', 'software', 'engineer')\n",
      "('data', 'scientist', 'data')\n",
      "('software', 'engineer', 'senior')\n",
      "('scientist', 'data', 'scientist')\n",
      "('engineer', 'senior', 'software')\n",
      "('sr', 'software', 'engineer')\n",
      "('software', 'engineer', 'software')\n",
      "('data', 'scientist', 'intern')\n",
      "('engineer', 'software', 'engineer')\n",
      "('vice', 'president', 'vice')\n",
      "\n",
      "\n",
      "OR----####----####----####\n",
      "\n",
      "\n",
      "('software', 'engineer')\n",
      "('data', 'scientist')\n",
      "('senior', 'software')\n",
      "('vice', 'president')\n",
      "('scientist', 'data')\n",
      "('engineer', 'senior')\n",
      "('data', 'analyst')\n",
      "('sr', 'software')\n",
      "('project', 'lead')\n",
      "('engineer', 'software')\n",
      "\n",
      "##############################################\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-669c4b8b6a71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;31m#this is for job title analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m     \u001b[0mperformance_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"indeed_scraped_data_science.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m     \u001b[1;31m#this is for calculating avg job experience for data science\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n##############################################\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-669c4b8b6a71>\u001b[0m in \u001b[0;36mperformance_evaluate\u001b[1;34m(input_file)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0mskills_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mtoken_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-bd331c5cd03f>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# write your code here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m#print(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \"\"\"\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[0;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No such file or directory: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "#for initial data filtering \n",
    "import preprocessing\n",
    "from re import compile\n",
    "#for sorting dictionary\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "\n",
    "def dateToSum(filename):\n",
    "    temp = 0\n",
    "    with open(filename, \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "\n",
    "        rows = [row[7] for row in reader]\n",
    "        tot = len(rows)\n",
    "        # print(tot)\n",
    "        for rro in rows:\n",
    "            if len(rro) > 0:\n",
    "                exp_len = ast.literal_eval(rro)\n",
    "                # print(exp_len)\n",
    "                for t in exp_len:\n",
    "                    tokens = nltk.word_tokenize(t)\n",
    "                    for i in range(len(tokens)):\n",
    "                        if tokens[i] == \"NA\":\n",
    "                            tokens[i] = \"\"\n",
    "                        elif tokens[i] == \"Present\":\n",
    "                            tokens[i] = \"2018\"\n",
    "                    rex = compile('[^0-9]')\n",
    "                    filteredData = [x for x in tokens if not rex.match(x)]\n",
    "\n",
    "                    if len(filteredData) == 2:\n",
    "                        temp = (int(filteredData[1]) - int(filteredData[0])) + temp\n",
    "\n",
    "        print(\"\\n\\nAverage Work Experience : \")\n",
    "        print(round(temp/tot))\n",
    "\n",
    "\n",
    "def performance_evaluate(input_file):\n",
    "    trigrams_list =[]\n",
    "    bigrams_list=[]\n",
    "    # write your code here\n",
    "    with open(input_file, \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        reader=csv.reader(f, delimiter=',')\n",
    "        # row[1] or second column is header of resume which states the current job position.\n",
    "        # Although we did data scraping for particular job positions, current job positions varied a lot\n",
    "        # in initial analysis phase- so even after scraping resumes for one particular position, we are making sure the\n",
    "        # actual job postion for which analysis will be done.\n",
    "        rows=[(row[1], row[5], row[4], row[10], row[3]) for row in reader]\n",
    "        \n",
    "   \n",
    "    #print(row_len)\n",
    "    for i in rows:\n",
    "        token_list = tokenize(i[0])\n",
    "        tmp_list = list(nltk.trigrams(token_list))\n",
    "        tmp_big_list = list(nltk.bigrams(token_list))\n",
    "        #print(tmp_big_list)\n",
    "        for j in tmp_list:\n",
    "            trigrams_list.append(j)\n",
    "        for l in tmp_big_list:\n",
    "            bigrams_list.append(l)\n",
    "    #print(bigrams_list)      \n",
    "    \n",
    "    #Here for job titles we cannot use NLTK's trigram collocation finder as it will filter out some important details.\n",
    "    #so finding it manually.\n",
    "    trigram_freq={}\n",
    "    bigram_freq={}\n",
    "    \n",
    "    for k in trigrams_list:\n",
    "        if k in trigram_freq:\n",
    "            trigram_freq[k]+=1\n",
    "        else:\n",
    "            trigram_freq[k]=1\n",
    "            \n",
    "    for k in bigrams_list:\n",
    "        if k in bigram_freq:\n",
    "            bigram_freq[k]+=1\n",
    "        else:\n",
    "            bigram_freq[k]=1\n",
    "        \n",
    "    # uncomment below print line to see the frequency for each trigram\n",
    "    # Sorting the dictionary below to easily see and get the top values when needed. For trigrams\n",
    "    sorted_freq = sorted(trigram_freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    #print(sorted_freq)\n",
    "    #print(\"Sorted by frequency - trigrams from heading\\n\\nSorted by Frequency Bigrams from heading\")\n",
    "    #for bigrams\n",
    "    bi_sorted_freq = sorted(bigram_freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    #print(bi_sorted_freq)\n",
    "    \n",
    "    # now the top most value is what we are looking for. and all our analysis will be for that position.\n",
    "    desired_position = sorted_freq[0][0]\n",
    "    print(\"\\n\\nThis analysis is for \",sorted_freq[0][0])\n",
    "    print(\"\\n##############################################\\n\")\n",
    "    #--------------------------------------------\n",
    "    # now for 5th column i.e. job titles.\n",
    "    #--------------------------------------------\n",
    "    exp_trigrams_list =[]\n",
    "    exp_bigrams_list=[]\n",
    "    for i in rows:\n",
    "        #second param is job exp\n",
    "        exp_list = tokenize(i[1])\n",
    "        #print(exp_list[0])\n",
    "        tmp_list = list(nltk.trigrams(exp_list))\n",
    "        tmp_big_list = list(nltk.bigrams(exp_list))\n",
    "        \n",
    "        for j in tmp_list:\n",
    "            exp_trigrams_list.append(j)\n",
    "        for l in tmp_big_list:\n",
    "            exp_bigrams_list.append(l)\n",
    "    \n",
    "    exp_trigram_freq={}\n",
    "    exp_bigram_freq={}\n",
    "    \n",
    "    for k in exp_trigrams_list:\n",
    "        if k in exp_trigram_freq:\n",
    "            exp_trigram_freq[k]+=1\n",
    "        else:\n",
    "            exp_trigram_freq[k]=1\n",
    "       \n",
    "    for k in exp_bigrams_list:\n",
    "        if k in exp_bigram_freq:\n",
    "            exp_bigram_freq[k]+=1\n",
    "        else:\n",
    "            exp_bigram_freq[k]=1\n",
    "      \n",
    "    \n",
    "    exp_sorted_freq = sorted(exp_trigram_freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    #print(exp_sorted_freq)\n",
    "    print(\"'input resume' can have more chances to become  \",desired_position,\" if it has following job experience terms\\n\")\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        print(exp_sorted_freq[i][0])\n",
    "    print(\"\\n\\nOR----####----####----####\\n\\n\")\n",
    "    #for bigrams\n",
    "    exp_bi_sorted_freq = sorted(exp_bigram_freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    #print(exp_bi_sorted_freq)\n",
    "    for i in range(0,10):\n",
    "        print(exp_bi_sorted_freq[i][0])\n",
    "    \n",
    "    \n",
    "    print(\"\\n##############################################\\n\")\n",
    "    #--------------------------------------------\n",
    "    # now for 4th column i.e. skills.\n",
    "    #--------------------------------------------\n",
    "    skills_list = []\n",
    "    for i in rows:\n",
    "        token_list = tokenize(i[2])\n",
    "        \n",
    "        for j in token_list:\n",
    "            skills_list.append(j)\n",
    "        \n",
    "    #print(len(skills_list))\n",
    "    skill_dict={}\n",
    "    for k in skills_list:\n",
    "        if k in skill_dict:\n",
    "            skill_dict[k]+=1\n",
    "        else:\n",
    "            skill_dict[k]=1\n",
    "            \n",
    "        \n",
    "    # uncomment below print line to see the frequency for each trigram\n",
    "    # Sorting the dictionary below to easily see and get the top values when needed. For trigrams\n",
    "    sorted_freq = sorted(skill_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    print(\"\\n\\nThese are preferred skills for the position.\\n\")\n",
    "    custom = [\"years\",\"year\",\"na\",\"less\",\"than\",\"and\"]\n",
    "    for i in range(0,35):\n",
    "        if(sorted_freq[i][0] not in custom):\n",
    "            print(sorted_freq[i][0], sorted_freq[i][1])\n",
    "    \n",
    "    print(\"\\n##############################################\\n\")\n",
    "    #--------------------------------------------\n",
    "    # now for 10th column i.e. skills.\n",
    "    #--------------------------------------------\n",
    "    skills_list = []\n",
    "    trigrams_list = []\n",
    "    for i in rows:\n",
    "        token_list = tokenize(i[3])\n",
    "        tmp_list = list(nltk.trigrams(token_list))\n",
    "        for j in tmp_list:\n",
    "            trigrams_list.append(j)\n",
    "        for k in token_list:\n",
    "            skills_list.append(k)\n",
    "        \n",
    "    \n",
    "    skill_dict={}\n",
    "    trigram_freq = {}\n",
    "    for l in trigrams_list:\n",
    "        if l in trigram_freq:\n",
    "            trigram_freq[l]+=1\n",
    "        else:\n",
    "            trigram_freq[l]=1\n",
    "    for k in skills_list:\n",
    "        if k in skill_dict:\n",
    "            skill_dict[k]+=1\n",
    "        else:\n",
    "            skill_dict[k]=1\n",
    "            \n",
    "        \n",
    "    # uncomment below print line to see the frequency for each trigram\n",
    "    # Sorting the dictionary below to easily see and get the top values when needed. For trigrams\n",
    "    sorted_freq = sorted(skill_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    tri_education = sorted(trigram_freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    print(\"\\n\\nThese are preferred Education details for the position.\\n\")\n",
    "    for j in range(0,8):\n",
    "        print(tri_education[j])\n",
    "    \n",
    "    custom = [\"years\",\"year\",\"na\",\"less\",\"than\",\"and\",\"in\",\"of\",\"data\",\"technology\"]\n",
    "    for i in range(0,8):\n",
    "        if(sorted_freq[i][0] not in custom):\n",
    "            print(sorted_freq[i])\n",
    "    \n",
    "    \n",
    "    print(\"\\n##############################################\\n\")\n",
    "    print(\"\\nWork authorization requirement for this position in USA\\n\")\n",
    "    #--------------------------------------------\n",
    "    # now for 10th column i.e. skills.\n",
    "    #--------------------------------------------\n",
    "    auth_list=[]\n",
    "    na = [\"NA\",\"na\"]\n",
    "    for i in rows:\n",
    "        auth_list.append(i[4])\n",
    "    auth_dict = {}\n",
    "    for i in auth_list:\n",
    "        if i in auth_dict:\n",
    "            auth_dict[i]+=1\n",
    "        else:\n",
    "            auth_dict[i]=1\n",
    "    sort_auth =  sorted(auth_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    for j in range(0,3):\n",
    "        if (sort_auth[j][0] not in na):\n",
    "            print(sort_auth[j][0])\n",
    "            \n",
    "            \n",
    "def description_analysis(input_file):\n",
    "    trigrams_list =[]\n",
    "    bigrams_list=[]\n",
    "    # write your code here\n",
    "    with open(input_file, \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        reader=csv.reader(f, delimiter=',')\n",
    "        rows=[(row[8]) for row in reader]\n",
    "    text=\". \".join(rows).lower()\n",
    "    \n",
    "    #print(row_len)\n",
    "    for i in rows:\n",
    "        token_list = tokenize(i)\n",
    "        tmp_list = list(nltk.trigrams(token_list))\n",
    "        tmp_big_list = list(nltk.bigrams(token_list))\n",
    "        #print(tmp_big_list)\n",
    "        for j in tmp_list:\n",
    "            trigrams_list.append(j)\n",
    "        for l in tmp_big_list:\n",
    "            bigrams_list.append(l)\n",
    "    #print(bigrams_list)      \n",
    "    finder = BigramCollocationFinder.from_words(\\\n",
    "        nltk.word_tokenize(text))\n",
    "    \n",
    "    finder.apply_freq_filter(5)\n",
    "    print(finder.nbest(bigram_measures.pmi, 10))\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    #this is for job title analysis\n",
    "    performance_evaluate(\"indeed_scraped_data_science.csv\")\n",
    "    #this is for calculating avg job experience for data science\n",
    "    print(\"\\n##############################################\\n\")\n",
    "    dateToSum(\"indeed_scraped_data_science.csv\")\n",
    "    print(\"\\n##############################################\\n\")\n",
    "    print(\"\\n\\nBelow are the preferred job experience terms for the given position.\\n\")\n",
    "    #this function will analyze the job description of all data science resumes.\n",
    "    description_analysis(\"indeed_scraped_data_science.csv\")\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
