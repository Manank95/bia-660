{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "with open(\"indeed_scraped_data_science.csv\", \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    text1 = [(row[8]) for row in reader if row[0]=='1']\n",
    "\n",
    "\n",
    "with open(\"indeed_scraped_data_science.csv\", \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    text2 = [(row[8]) for row in reader if row[0]=='2']\n",
    "    \n",
    "with open(\"indeed_scraped_data_science.csv\", \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    text3 = [(row[8]) for row in reader if row[0]=='3']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "from re import compile\n",
    "#for sorting dictionary\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens=[]\n",
    "    # write your code here\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    #print(text)\n",
    "    #pattern = r'\\w'\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip(string.punctuation) for token in tokens]\n",
    "    \n",
    "    tokens = [token for token in tokens if re.match(\"^[A-Za-z_-]*$\", token)]\n",
    "    #print(tokens)\n",
    "    # to remove punctuations from begging and starting of the tokens \n",
    "    \n",
    "    # now removing extra empty characters from tokens\n",
    "    tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "    tokens = [token for token in tokens if len(token)>1]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a Keras tokenizer\n",
    "\n",
    "MAX_NB_WORDS=8000\n",
    "# documents are quite long in the dataset\n",
    "MAX_DOC_LEN=1000\n",
    "\n",
    "tokenizer1 = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer1.fit_on_texts(text1)\n",
    "\n",
    "tokenizer2 = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer2.fit_on_texts(text2)\n",
    "\n",
    "tokenizer3 = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer3.fit_on_texts(text3)\n",
    "\n",
    "input_tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "input_tokenizer.fit_on_texts(text1[0])\n",
    "\n",
    "with open(\"project_lead.csv\", \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "    reader=csv.reader(f, delimiter=',')\n",
    "    rows=[(row[8]) for row in reader]\n",
    "    \n",
    "#print(rows[0])\n",
    "#input_tokens = []\n",
    "\n",
    "input_tokens = tokenize(rows[10])\n",
    "\n",
    "   \n",
    "#print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             freq\n",
      "description   323\n",
      "fis            23\n",
      "provides      190\n",
      "financial     425\n",
      "software      637\n",
      "   word_freq  count\n",
      "1          1   3129\n",
      "0          2   3476\n",
      "2          3   1239\n",
      "3          4   1201\n",
      "5          5    529\n",
      "          freq\n",
      "'product    25\n",
      "package    122\n",
      "planner     23\n",
      "united      32\n",
      "parcel       4\n",
      "   word_freq  count\n",
      "0          1   7634\n",
      "1          2   4459\n",
      "2          3   1506\n",
      "3          4   1253\n",
      "4          5    642\n",
      "              freq\n",
      "'taught          6\n",
      "students        64\n",
      "the           2589\n",
      "introduction     6\n",
      "to            2587\n",
      "   word_freq  count\n",
      "0          1   5065\n",
      "1          2   1752\n",
      "2          3    783\n",
      "3          4    521\n",
      "4          5    380\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame.from_dict(tokenizer1.word_counts, orient=\"index\")\n",
    "df.columns=['freq']\n",
    "print(df.head())\n",
    "df=df['freq'].value_counts().reset_index()\n",
    "df.columns=['word_freq','count']\n",
    "\n",
    "df=df.sort_values(by='word_freq')\n",
    "#print(tokenizer1.word_counts)\n",
    "print(df.head())\n",
    "\n",
    "df=pd.DataFrame.from_dict(tokenizer2.word_counts, orient=\"index\")\n",
    "df.columns=['freq']\n",
    "print(df.head())\n",
    "df=df['freq'].value_counts().reset_index()\n",
    "df.columns=['word_freq','count']\n",
    "\n",
    "df=df.sort_values(by='word_freq')\n",
    "#print(tokenizer1.word_counts)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df=pd.DataFrame.from_dict(tokenizer3.word_counts, orient=\"index\")\n",
    "df.columns=['freq']\n",
    "print(df.head())\n",
    "df=df['freq'].value_counts().reset_index()\n",
    "df.columns=['word_freq','count']\n",
    "\n",
    "df=df.sort_values(by='word_freq')\n",
    "#print(tokenizer1.word_counts)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "#for initial data filtering \n",
    "import preprocessing\n",
    "from re import compile\n",
    "#for sorting dictionary\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dateToSum(filename):\n",
    "    temp = 0\n",
    "    with open(filename, \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "\n",
    "        rows = [row[7] for row in reader]\n",
    "        tot = len(rows)\n",
    "        # print(tot)\n",
    "        for rro in rows:\n",
    "            if len(rro) > 0:\n",
    "                exp_len = ast.literal_eval(rro)\n",
    "                # print(exp_len)\n",
    "                for t in exp_len:\n",
    "                    tokens = nltk.word_tokenize(t)\n",
    "                    for i in range(len(tokens)):\n",
    "                        if tokens[i] == \"NA\":\n",
    "                            tokens[i] = \"\"\n",
    "                        elif tokens[i] == \"Present\":\n",
    "                            tokens[i] = \"2018\"\n",
    "                    rex = compile('[^0-9]')\n",
    "                    filteredData = [x for x in tokens if not rex.match(x)]\n",
    "\n",
    "                    if len(filteredData) == 2:\n",
    "                        temp = (int(filteredData[1]) - int(filteredData[0])) + temp\n",
    "\n",
    "        print(\"\\n\\nAverage Work Experience : \")\n",
    "        print(round(temp/tot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance_evaluate(input_file):\n",
    "    trigrams_list =[]\n",
    "    bigrams_list=[]\n",
    "    # write your code here\n",
    "    with open(input_file, \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        reader=csv.reader(f, delimiter=',')\n",
    "        # row[1] or second column is header of resume which states the current job position.\n",
    "        # Although we did data scraping for particular job positions, current job positions varied a lot\n",
    "        # in initial analysis phase- so even after scraping resumes for one particular position, we are making sure the\n",
    "        # actual job postion for which analysis will be done.\n",
    "        rows=[(row[1], row[5], row[4], row[10], row[3],row[8]) for row in reader]\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    #print(row_len)\n",
    "    for i in rows:\n",
    "        token_list = tokenize(i[0])\n",
    "        tmp_list = list(nltk.trigrams(token_list))\n",
    "        tmp_big_list = list(nltk.bigrams(token_list))\n",
    "        #print(tmp_big_list)\n",
    "        for j in tmp_list:\n",
    "            trigrams_list.append(j)\n",
    "        for l in tmp_big_list:\n",
    "            bigrams_list.append(l)\n",
    "    #print(bigrams_list)      \n",
    "    \n",
    "    #Here for job titles we cannot use NLTK's trigram collocation finder as it will filter out some important details.\n",
    "    #so finding it manually.\n",
    "    trigram_freq={}\n",
    "    bigram_freq={}\n",
    "    \n",
    "    for k in trigrams_list:\n",
    "        if k in trigram_freq:\n",
    "            trigram_freq[k]+=1\n",
    "        else:\n",
    "            trigram_freq[k]=1\n",
    "            \n",
    "    for k in bigrams_list:\n",
    "        if k in bigram_freq:\n",
    "            bigram_freq[k]+=1\n",
    "        else:\n",
    "            bigram_freq[k]=1\n",
    "        \n",
    "    # uncomment below print line to see the frequency for each trigram\n",
    "    # Sorting the dictionary below to easily see and get the top values when needed. For trigrams\n",
    "    sorted_freq = sorted(trigram_freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    #print(sorted_freq)\n",
    "    #print(\"Sorted by frequency - trigrams from heading\\n\\nSorted by Frequency Bigrams from heading\")\n",
    "    #for bigrams\n",
    "    bi_sorted_freq = sorted(bigram_freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    #print(bi_sorted_freq)\n",
    "    \n",
    "    # now the top most value is what we are looking for. and all our analysis will be for that position.\n",
    "    desired_position = sorted_freq[0][0]\n",
    "    print(\"\\n\\nThis analysis is for \",sorted_freq[0][0])\n",
    "    print(\"\\n##############################################\\n\")\n",
    "    #--------------------------------------------\n",
    "    # now for 5th column i.e. job titles.\n",
    "    #--------------------------------------------\n",
    "    exp_trigrams_list =[]\n",
    "    exp_bigrams_list=[]\n",
    "    for i in rows:\n",
    "        #second param is job exp\n",
    "        exp_list = tokenize(i[1])\n",
    "        #print(exp_list[0])\n",
    "        tmp_list = list(nltk.trigrams(exp_list))\n",
    "        tmp_big_list = list(nltk.bigrams(exp_list))\n",
    "        \n",
    "        for j in tmp_list:\n",
    "            exp_trigrams_list.append(j)\n",
    "        for l in tmp_big_list:\n",
    "            exp_bigrams_list.append(l)\n",
    "    \n",
    "    exp_trigram_freq={}\n",
    "    exp_bigram_freq={}\n",
    "    \n",
    "    for k in exp_trigrams_list:\n",
    "        if k in exp_trigram_freq:\n",
    "            exp_trigram_freq[k]+=1\n",
    "        else:\n",
    "            exp_trigram_freq[k]=1\n",
    "       \n",
    "    for k in exp_bigrams_list:\n",
    "        if k in exp_bigram_freq:\n",
    "            exp_bigram_freq[k]+=1\n",
    "        else:\n",
    "            exp_bigram_freq[k]=1\n",
    "      \n",
    "    \n",
    "    exp_sorted_freq = sorted(exp_trigram_freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    #print(exp_sorted_freq)\n",
    "    print(\"'input resume' can have more chances to become  \",desired_position,\" if it has following job experience terms\\n\")\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        print(exp_sorted_freq[i][0])\n",
    "    print(\"\\n\\nOR----####----####----####\\n\\n\")\n",
    "    #for bigrams\n",
    "    exp_bi_sorted_freq = sorted(exp_bigram_freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    #print(exp_bi_sorted_freq)\n",
    "    for i in range(0,10):\n",
    "        print(exp_bi_sorted_freq[i][0])\n",
    "    \n",
    "    \n",
    "    print(\"\\n##############################################\\n\")\n",
    "    #--------------------------------------------\n",
    "    # now for 4th column i.e. skills.\n",
    "    #--------------------------------------------\n",
    "    skills_list = []\n",
    "    for i in rows:\n",
    "        token_list = tokenize(i[2])\n",
    "        \n",
    "        for j in token_list:\n",
    "            skills_list.append(j)\n",
    "        \n",
    "    #print(len(skills_list))\n",
    "    skill_dict={}\n",
    "    for k in skills_list:\n",
    "        if k in skill_dict:\n",
    "            skill_dict[k]+=1\n",
    "        else:\n",
    "            skill_dict[k]=1\n",
    "            \n",
    "        \n",
    "    # uncomment below print line to see the frequency for each trigram\n",
    "    # Sorting the dictionary below to easily see and get the top values when needed. For trigrams\n",
    "    sorted_freq = sorted(skill_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    print(\"\\n\\nThese are preferred skills for the position.\\n\")\n",
    "    custom = [\"years\",\"year\",\"na\",\"less\",\"than\",\"and\"]\n",
    "    for i in range(0,35):\n",
    "        if(sorted_freq[i][0] not in custom):\n",
    "            print(sorted_freq[i][0], sorted_freq[i][1])\n",
    "            \n",
    "            \n",
    "            \n",
    "    print(\"###############  Description top words  ################3\")\n",
    "    \n",
    "    skills_list = []\n",
    "    for i in rows:\n",
    "        token_list = tokenize(i[5])\n",
    "        \n",
    "        for j in token_list:\n",
    "            skills_list.append(j)\n",
    "    \n",
    "    \n",
    "        \n",
    "    #print(len(skills_list))\n",
    "    skill_dict={}\n",
    "    for k in skills_list:\n",
    "        if k in skill_dict:\n",
    "            skill_dict[k]+=1\n",
    "        else:\n",
    "            skill_dict[k]=1\n",
    "            \n",
    "        \n",
    "    # uncomment below print line to see the frequency for each trigram\n",
    "    # Sorting the dictionary below to easily see and get the top values when needed. For trigrams\n",
    "    sorted_freq = sorted(skill_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    print(\"\\n\\nThese are preferred description words for the position.\\n\")\n",
    "    custom = [\"years\",\"year\",\"na\",\"less\",\"than\",\"and\",\"using\"]\n",
    "    return_list = []\n",
    "    for i in range(0,len(sorted_freq)-5):\n",
    "        return_list.append(sorted_freq[i])\n",
    "        \n",
    "    for i in range(0,35):\n",
    "        if(sorted_freq[i][0] not in custom):\n",
    "            print(sorted_freq[i][0], sorted_freq[i][1])\n",
    "    \n",
    "    \n",
    "    print(\"\\n##############################################\\n\")\n",
    "    #--------------------------------------------\n",
    "    # now for 10th column i.e. skills.\n",
    "    #--------------------------------------------\n",
    "    skills_list = []\n",
    "    trigrams_list = []\n",
    "    for i in rows:\n",
    "        token_list = tokenize(i[3])\n",
    "        tmp_list = list(nltk.trigrams(token_list))\n",
    "        for j in tmp_list:\n",
    "            trigrams_list.append(j)\n",
    "        for k in token_list:\n",
    "            skills_list.append(k)\n",
    "        \n",
    "    \n",
    "    skill_dict={}\n",
    "    trigram_freq = {}\n",
    "    for l in trigrams_list:\n",
    "        if l in trigram_freq:\n",
    "            trigram_freq[l]+=1\n",
    "        else:\n",
    "            trigram_freq[l]=1\n",
    "    for k in skills_list:\n",
    "        if k in skill_dict:\n",
    "            skill_dict[k]+=1\n",
    "        else:\n",
    "            skill_dict[k]=1\n",
    "            \n",
    "        \n",
    "    # uncomment below print line to see the frequency for each trigram\n",
    "    # Sorting the dictionary below to easily see and get the top values when needed. For trigrams\n",
    "    sorted_freq = sorted(skill_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    tri_education = sorted(trigram_freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    print(\"\\n\\nThese are preferred Education details for the position.\\n\")\n",
    "    for j in range(0,8):\n",
    "        print(tri_education[j])\n",
    "    \n",
    "    custom = [\"years\",\"year\",\"na\",\"less\",\"than\",\"and\",\"in\",\"of\",\"data\",\"technology\"]\n",
    "    for i in range(0,8):\n",
    "        if(sorted_freq[i][0] not in custom):\n",
    "            print(sorted_freq[i])\n",
    "    \n",
    "    \n",
    "    print(\"\\n##############################################\\n\")\n",
    "    print(\"\\nWork authorization requirement for this position in USA\\n\")\n",
    "    #--------------------------------------------\n",
    "    # now for 10th column i.e. skills.\n",
    "    #--------------------------------------------\n",
    "    auth_list=[]\n",
    "    na = [\"NA\",\"na\"]\n",
    "    for i in rows:\n",
    "        auth_list.append(i[4])\n",
    "    auth_dict = {}\n",
    "    for i in auth_list:\n",
    "        if i in auth_dict:\n",
    "            auth_dict[i]+=1\n",
    "        else:\n",
    "            auth_dict[i]=1\n",
    "    sort_auth =  sorted(auth_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    for j in range(0,3):\n",
    "        if (sort_auth[j][0] not in na):\n",
    "            print(sort_auth[j][0])\n",
    "            \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def description_analysis(input_file):\n",
    "    trigrams_list =[]\n",
    "    bigrams_list=[]\n",
    "    # write your code here\n",
    "    with open(input_file, \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        reader=csv.reader(f, delimiter=',')\n",
    "        rows=[(row[8]) for row in reader]\n",
    "    text=\". \".join(rows).lower()\n",
    "    \n",
    "    #print(row_len)\n",
    "    for i in rows:\n",
    "        token_list = tokenize(i)\n",
    "        tmp_list = list(nltk.trigrams(token_list))\n",
    "        tmp_big_list = list(nltk.bigrams(token_list))\n",
    "        #print(tmp_big_list)\n",
    "        for j in tmp_list:\n",
    "            trigrams_list.append(j)\n",
    "        for l in tmp_big_list:\n",
    "            bigrams_list.append(l)\n",
    "    #print(bigrams_list)      \n",
    "    finder = BigramCollocationFinder.from_words(\\\n",
    "        nltk.word_tokenize(text))\n",
    "    print(token_list[0:10])\n",
    "    finder.apply_freq_filter(5)\n",
    "    print(finder.nbest(bigram_measures.pmi, 10))\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    #this is for job title analysis\n",
    "    file1 = \"data_science.csv\"\n",
    "    file2 = \"project_lead.csv\"\n",
    "    file3 = \"vice_president.csv\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This analysis is for  ('data', 'scientist', 'intern')\n",
      "\n",
      "##############################################\n",
      "\n",
      "'input resume' can have more chances to become   ('data', 'scientist', 'intern')  if it has following job experience terms\n",
      "\n",
      "('data', 'scientist', 'data')\n",
      "('scientist', 'data', 'scientist')\n",
      "('data', 'scientist', 'intern')\n",
      "('scientist', 'data', 'analyst')\n",
      "('data', 'analyst', 'data')\n",
      "('analyst', 'data', 'analyst')\n",
      "('senior', 'data', 'scientist')\n",
      "('data', 'analyst', 'intern')\n",
      "('scientist', 'data', 'modeler')\n",
      "('graduate', 'research', 'assistant')\n",
      "\n",
      "\n",
      "OR----####----####----####\n",
      "\n",
      "\n",
      "('data', 'scientist')\n",
      "('scientist', 'data')\n",
      "('data', 'analyst')\n",
      "('scientist', 'intern')\n",
      "('research', 'assistant')\n",
      "('analyst', 'data')\n",
      "('data', 'modeler')\n",
      "('software', 'engineer')\n",
      "('intern', 'data')\n",
      "('engineer', 'data')\n",
      "\n",
      "##############################################\n",
      "\n",
      "\n",
      "\n",
      "These are preferred skills for the position.\n",
      "\n",
      "python 398\n",
      "sql 383\n",
      "data 365\n",
      "learning 245\n",
      "analysis 201\n",
      "machine 193\n",
      "hadoop 178\n",
      "excel 125\n",
      "apache 117\n",
      "java 116\n",
      "sas 101\n",
      "matlab 83\n",
      "database 80\n",
      "tableau 76\n",
      "aws 71\n",
      "mining 70\n",
      "business 69\n",
      "visualization 68\n",
      "linux 63\n",
      "microsoft 61\n",
      "ms 60\n",
      "office 57\n",
      "analytics 55\n",
      "intelligence 55\n",
      "spark 52\n",
      "statistics 52\n",
      "statistical 51\n",
      "css 49\n",
      "git 48\n",
      "management 48\n",
      "html 46\n",
      "###############  Description top words  ################3\n",
      "\n",
      "\n",
      "These are preferred description words for the position.\n",
      "\n",
      "data 15654\n",
      "analysis 3514\n",
      "business 2842\n",
      "sql 2418\n",
      "developed 2277\n",
      "python 2271\n",
      "models 2217\n",
      "used 2155\n",
      "learning 2116\n",
      "model 1987\n",
      "machine 1809\n",
      "worked 1578\n",
      "various 1525\n",
      "project 1522\n",
      "team 1464\n",
      "performed 1441\n",
      "design 1381\n",
      "development 1337\n",
      "reports 1246\n",
      "database 1245\n",
      "requirements 1228\n",
      "regression 1222\n",
      "created 1200\n",
      "system 1185\n",
      "based 1178\n",
      "statistical 1146\n",
      "implemented 1146\n",
      "environment 1124\n",
      "management 1105\n",
      "tableau 1078\n",
      "analytics 1050\n",
      "algorithms 1043\n",
      "hadoop 999\n",
      "server 984\n",
      "\n",
      "##############################################\n",
      "\n",
      "\n",
      "\n",
      "These are preferred Education details for the position.\n",
      "\n",
      "(('science', 'computer', 'science'), 73)\n",
      "(('computer', 'science', 'engineering'), 52)\n",
      "(('science', 'data', 'science'), 47)\n",
      "(('master', 'science', 'computer'), 44)\n",
      "(('engineering', 'computer', 'science'), 33)\n",
      "(('computer', 'science', 'bachelor'), 31)\n",
      "(('data', 'science', 'bachelor'), 31)\n",
      "(('computer', 'science', 'computer'), 31)\n",
      "('science', 807)\n",
      "('engineering', 510)\n",
      "('bachelor', 464)\n",
      "('master', 391)\n",
      "('computer', 328)\n",
      "('statistics', 171)\n",
      "('mathematics', 139)\n",
      "\n",
      "##############################################\n",
      "\n",
      "\n",
      "Work authorization requirement for this position in USA\n",
      "\n",
      "Authorized to work in the US for any employer\n",
      "Sponsorship required to work in the US\n"
     ]
    }
   ],
   "source": [
    "training_data_science = performance_evaluate(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This analysis is for  ('senior', 'software', 'engineer')\n",
      "\n",
      "##############################################\n",
      "\n",
      "'input resume' can have more chances to become   ('senior', 'software', 'engineer')  if it has following job experience terms\n",
      "\n",
      "('senior', 'software', 'engineer')\n",
      "('software', 'engineer', 'senior')\n",
      "('engineer', 'senior', 'software')\n",
      "('sr', 'software', 'engineer')\n",
      "('software', 'engineer', 'software')\n",
      "('engineer', 'software', 'engineer')\n",
      "('software', 'engineer', 'project')\n",
      "('software', 'engineer', 'sr')\n",
      "('engineer', 'project', 'lead')\n",
      "('engineer', 'sr', 'software')\n",
      "\n",
      "\n",
      "OR----####----####----####\n",
      "\n",
      "\n",
      "('software', 'engineer')\n",
      "('senior', 'software')\n",
      "('engineer', 'senior')\n",
      "('sr', 'software')\n",
      "('project', 'lead')\n",
      "('engineer', 'software')\n",
      "('engineer', 'project')\n",
      "('project', 'manager')\n",
      "('software', 'developer')\n",
      "('engineer', 'sr')\n",
      "\n",
      "##############################################\n",
      "\n",
      "\n",
      "\n",
      "These are preferred skills for the position.\n",
      "\n",
      "sql 138\n",
      "java 78\n",
      "server 65\n",
      "oracle 61\n",
      "net 53\n",
      "web 51\n",
      "javascript 42\n",
      "ms 39\n",
      "asp 39\n",
      "linux 34\n",
      "visual 33\n",
      "development 33\n",
      "html 32\n",
      "services 31\n",
      "jquery 27\n",
      "windows 26\n",
      "xml 24\n",
      "apache 24\n",
      "software 24\n",
      "design 24\n",
      "unix 22\n",
      "database 22\n",
      "perl 21\n",
      "python 20\n",
      "android 19\n",
      "studio 18\n",
      "css 18\n",
      "management 16\n",
      "spring 14\n",
      "agile 14\n",
      "mvc 14\n",
      "###############  Description top words  ################3\n",
      "\n",
      "\n",
      "These are preferred description words for the position.\n",
      "\n",
      "system 3100\n",
      "application 2792\n",
      "development 2782\n",
      "data 2682\n",
      "software 2521\n",
      "developed 2499\n",
      "project 2423\n",
      "design 2298\n",
      "web 2093\n",
      "server 1999\n",
      "team 1829\n",
      "management 1712\n",
      "sql 1670\n",
      "used 1617\n",
      "business 1435\n",
      "services 1376\n",
      "testing 1369\n",
      "test 1327\n",
      "applications 1287\n",
      "support 1267\n",
      "designed 1259\n",
      "database 1230\n",
      "new 1205\n",
      "based 1171\n",
      "java 1113\n",
      "code 1107\n",
      "requirements 1048\n",
      "windows 1041\n",
      "implemented 1034\n",
      "oracle 1014\n",
      "user 1005\n",
      "involved 1005\n",
      "product 966\n",
      "visual 966\n",
      "\n",
      "##############################################\n",
      "\n",
      "\n",
      "\n",
      "These are preferred Education details for the position.\n",
      "\n",
      "(('science', 'computer', 'science'), 53)\n",
      "(('bachelor', 'science', 'computer'), 32)\n",
      "(('computer', 'science', 'engineering'), 19)\n",
      "(('engineering', 'computer', 'science'), 18)\n",
      "(('computer', 'science', 'computer'), 18)\n",
      "(('computer', 'science', 'bachelor'), 17)\n",
      "(('master', 'science', 'computer'), 17)\n",
      "(('computer', 'information', 'systems'), 16)\n",
      "('science', 343)\n",
      "('computer', 318)\n",
      "('bachelor', 174)\n",
      "('engineering', 162)\n",
      "('master', 88)\n",
      "('information', 58)\n",
      "('systems', 44)\n",
      "\n",
      "##############################################\n",
      "\n",
      "\n",
      "Work authorization requirement for this position in USA\n",
      "\n",
      "Authorized to work in the US for any employer\n",
      "Sponsorship required to work in the US\n"
     ]
    }
   ],
   "source": [
    "    training_software_engineer = performance_evaluate(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This analysis is for  ('vice', 'president', 'valley')\n",
      "\n",
      "##############################################\n",
      "\n",
      "'input resume' can have more chances to become   ('vice', 'president', 'valley')  if it has following job experience terms\n",
      "\n",
      "('vice', 'president', 'vice')\n",
      "('president', 'vice', 'president')\n",
      "('assistant', 'vice', 'president')\n",
      "('vice', 'president', 'senior')\n",
      "('vice', 'president', 'director')\n",
      "('vice', 'president', 'sales')\n",
      "('vice', 'president', 'assistant')\n",
      "('vice', 'president', 'na')\n",
      "('president', 'assistant', 'vice')\n",
      "('vice', 'president', 'associate')\n",
      "\n",
      "\n",
      "OR----####----####----####\n",
      "\n",
      "\n",
      "('vice', 'president')\n",
      "('president', 'vice')\n",
      "('assistant', 'vice')\n",
      "('president', 'senior')\n",
      "('president', 'director')\n",
      "('president', 'sales')\n",
      "('president', 'assistant')\n",
      "('president', 'na')\n",
      "('president', 'associate')\n",
      "('project', 'manager')\n",
      "\n",
      "##############################################\n",
      "\n",
      "\n",
      "\n",
      "These are preferred skills for the position.\n",
      "\n",
      "management 90\n",
      "microsoft 68\n",
      "office 62\n",
      "excel 54\n",
      "development 34\n",
      "sales 33\n",
      "word 29\n",
      "powerpoint 25\n",
      "customer 24\n",
      "project 23\n",
      "marketing 23\n",
      "analysis 22\n",
      "business 22\n",
      "operations 22\n",
      "planning 21\n",
      "financial 21\n",
      "ms 19\n",
      "service 19\n",
      "leadership 17\n",
      "training 15\n",
      "strategic 14\n",
      "skills 14\n",
      "budget 13\n",
      "data 12\n",
      "problem 11\n",
      "relationship 11\n",
      "sql 10\n",
      "accounting 10\n",
      "excellent 10\n",
      "quickbooks 9\n",
      "solving 9\n",
      "###############  Description top words  ################3\n",
      "\n",
      "\n",
      "These are preferred description words for the position.\n",
      "\n",
      "business 585\n",
      "management 566\n",
      "sales 528\n",
      "new 512\n",
      "team 413\n",
      "including 397\n",
      "development 357\n",
      "responsible 333\n",
      "managed 318\n",
      "clients 314\n",
      "operations 289\n",
      "financial 282\n",
      "company 281\n",
      "services 273\n",
      "customer 272\n",
      "developed 266\n",
      "client 260\n",
      "service 245\n",
      "data 234\n",
      "process 217\n",
      "marketing 203\n",
      "project 202\n",
      "support 195\n",
      "analysis 193\n",
      "market 184\n",
      "training 177\n",
      "design 168\n",
      "million 166\n",
      "program 166\n",
      "implemented 164\n",
      "projects 160\n",
      "system 159\n",
      "reporting 157\n",
      "created 157\n",
      "\n",
      "##############################################\n",
      "\n",
      "\n",
      "\n",
      "These are preferred Education details for the position.\n",
      "\n",
      "(('bachelor', 'science', 'business'), 18)\n",
      "(('high', 'school', 'diploma'), 17)\n",
      "(('bachelor', 'business', 'administration'), 16)\n",
      "(('business', 'administration', 'finance'), 13)\n",
      "(('master', 'business', 'administration'), 12)\n",
      "(('science', 'business', 'administration'), 10)\n",
      "(('science', 'mechanical', 'engineering'), 9)\n",
      "(('high', 'school', 'equivalent'), 9)\n",
      "('bachelor', 177)\n",
      "('business', 127)\n",
      "('science', 120)\n",
      "('administration', 80)\n",
      "('arts', 57)\n",
      "('master', 51)\n",
      "('finance', 49)\n",
      "\n",
      "##############################################\n",
      "\n",
      "\n",
      "Work authorization requirement for this position in USA\n",
      "\n",
      "Authorized to work in the US for any employer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    training_vice_president = performance_evaluate(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################################\n",
      "\n",
      "14021\n",
      "Probability of the input resume of being data science is: \n",
      "\n",
      "##############################################\n",
      "\n",
      "\n",
      "\n",
      "Below are the preferred job experience terms for the given position.\n",
      "\n",
      "['utilize', 'lcms', 'analyze', 'target', 'analytes', 'assist', 'diagnosis', 'biochemical', 'disorders', 'utilize']\n",
      "[('diabetic', 'retinopathy'), ('dow', 'jones'), ('higgs', 'boson'), ('abn', 'amro'), ('learn/', 'scipy/'), ('rac/10g', 'rac/9i'), ('scipy/', 'numpy/'), ('seed/production', 'fermenters'), ('tremendous', 'pride'), ('lin', 'z.')]\n",
      "##################3\n",
      "['lmm', 'legacy', 'managed', 'market', 'cm', 'customer', 'master', 'remediation', 'project', 'multiple']\n",
      "[('ezx', 'xwindows'), ('jd', 'edwards'), ('proving', 'grounds'), ('rhode', 'island'), ('basking', 'ridge'), ('condor', 'arinc-429'), ('td', 'ameritrade'), ('daimler', 'chrysler'), ('loosely', 'coupled'), ('checkfree', 'apl')]\n",
      "####################\n",
      "['cmu', 'engineering', 'club', 'formula', 'sae', 'team', 'coordinate', 'colorado', 'mesa', 'university']\n",
      "[('hong', 'kong'), ('los', 'angeles'), ('merrill', 'lynch'), ('richard', 'ellis'), ('subject', 'matter'), ('st.', 'louis'), ('fed', 'ex'), ('cold', 'calling'), ('square', 'feet'), ('@', 'gmail.com')]\n"
     ]
    }
   ],
   "source": [
    "    #this is for calculating avg job experience for data science\n",
    "    print(\"\\n##############################################\\n\")\n",
    "    #dateToSum(\"indeed_scraped_data_science.csv\")\n",
    "\n",
    "    tot = len(training_data_science)\n",
    "    #print(input_tokens)\n",
    "    #print(len(training_data_science))\n",
    "    print(\"\\n##############################################\\n\")\n",
    "    \n",
    "    #these functions will analyze the job description of all positions from training dataset.\n",
    "    \n",
    "    print(\"\\nBelow are the preferred job experience terms for the data science position.\\n\")\n",
    "    description_analysis(file1)\n",
    "    print(\"\\n###################\")\n",
    "    \n",
    "    print(\"\\nBelow are the preferred job experience terms for the project lead position.\\n\")\n",
    "    description_analysis(file2)\n",
    "    print(\"####################\")\n",
    "    \n",
    "    print(\"\\nBelow are the preferred job experience terms for the vice president position.\\n\")\n",
    "    description_analysis(file3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chances of input resume to be a Data Scientist (CNN on job description) are: \n",
      "32.10631807763958\n",
      "Chances of input resume to be a Project Lead (CNN on job description) are: \n",
      "38.37256176994119\n",
      "Chances of input resume to be a Vice President (CNN on job description) are: \n",
      "29.521120152419222\n"
     ]
    }
   ],
   "source": [
    "def compare_prob(input_tokens,training_data_science):\n",
    "    count = 0\n",
    "    for itk in training_data_science:\n",
    "        j = 0\n",
    "        itk = itk[0].lower()\n",
    "        #print(itk)\n",
    "        for tdk in input_tokens:\n",
    "            j = j + 1\n",
    "            tdk = tdk.lower()\n",
    "            #print(tdk)\n",
    "            if(itk == tdk):\n",
    "                count = count + (tot-j)    \n",
    "\n",
    "    return count\n",
    "\n",
    "tmp1 = compare_prob(input_tokens,training_data_science)\n",
    "tmp2 = compare_prob(input_tokens,training_software_engineer)\n",
    "tmp3 = compare_prob(input_tokens,training_vice_president)\n",
    "\n",
    "tot = tmp1+tmp2+tmp3\n",
    "\n",
    "#input sample is taken from with the skills of Project Lead. \n",
    "\n",
    "print(\"Chances of input resume to be a Data Scientist (CNN on job description) are: \")\n",
    "print(tmp1/tot * 100)\n",
    "\n",
    "print(\"\\n\\nChances of input resume to be a Project Lead (CNN on job description) are: \")\n",
    "print(tmp2/tot * 100)\n",
    "\n",
    "print(\"\\n\\nChances of input resume to be a Vice President (CNN on job description) are: \")\n",
    "print(tmp3/tot * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
